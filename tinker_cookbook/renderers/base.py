"""
Use viz_sft_dataset to visualize the output of different renderers. E.g.,
    python -m tinker_cookbook.supervised.viz_sft_dataset dataset_path=Tulu3Builder renderer_name=role_colon
"""

import json
import logging
import re
import urllib.request
from datetime import datetime
from enum import StrEnum
from typing import NotRequired, Optional, TypedDict, Literal, Protocol, cast
from PIL import Image

import tinker
import torch
import pydantic

import io

from tinker_cookbook.tokenizer_utils import Tokenizer
from tinker_cookbook.image_processing_utils import ImageProcessor


class StrictBase(pydantic.BaseModel):
    """
    Pydantic base class that's immutable and doesn't silently ignore extra fields.
    """

    model_config = pydantic.ConfigDict(frozen=True, extra="forbid")

    def __str__(self) -> str:
        return repr(self)


class ToolCall(StrictBase):
    """
    Structured tool invocation following OpenAI/kosong format.

    This represents a request to invoke a tool/function. The structure follows
    the OpenAI function calling format for compatibility with various LLM APIs.

    Example:
        tool_call = ToolCall(
            function=ToolCall.FunctionBody(
                name="search",
                arguments='{"query_list": ["python async", "pydantic validation"]}'
            ),
            id="call_abc123"
        )
    """

    class FunctionBody(pydantic.BaseModel):
        """
        Tool call function body containing the tool name and arguments.

        The arguments field must be a valid JSON string that will be parsed
        by the tool implementation.
        """

        name: str
        """The name of the tool to be called."""
        arguments: str
        """Arguments of the tool call in JSON string format."""

    type: Literal["function"] = "function"
    """Tool call type, must be 'function' for compatibility."""

    id: str | None = None
    """Optional unique identifier for tracking this specific tool call."""

    function: FunctionBody
    """The function body containing tool name and arguments."""


class ToolOk(StrictBase):
    """
    Successful tool execution result.

    Used to indicate that a tool call completed successfully, with
    the main output and optional metadata fields.
    """

    output: str
    """The main output/result from the tool execution."""

    message: str = ""
    """Optional human-readable message about the execution."""

    brief: str = ""
    """Optional brief summary of the result for logging."""


class ToolError(StrictBase):
    """
    Tool execution error result.

    Used to indicate that a tool call failed or encountered an error,
    with details about what went wrong.
    """

    output: str = ""
    """Any partial output that was generated before the error."""

    message: str = ""
    """Error message describing what went wrong."""

    brief: str = ""
    """Brief error summary for logging."""


ToolReturnType = ToolOk | ToolError


class ToolResult(StrictBase):
    """
    Complete tool execution result with tracking ID.

    Wraps the actual result (ToolOk or ToolError) with the corresponding
    tool call ID for correlation in multi-tool scenarios.

    Note: This class is defined for future use in handling multiple
    concurrent tool calls with result correlation.
    """

    tool_call_id: str | None
    """ID of the tool call this result corresponds to."""

    result: ToolReturnType
    """The actual execution result (success or error)."""


class TextPart(TypedDict):
    """
    Container for a text part in a multimodal message.

    Args:

    type: Literal['text']
        The type of the content part, which must be text in this case.
    text: str
        The string content of the content part.
    """

    type: Literal["text"]
    text: str


class ImagePart(TypedDict):
    """
    Container for an image part in a multimodal message.

    Args:

    type: Literal['image']
        The type of the content part, which must be image in this case.
    image: str | Image.Image
        Either a url, data URL, or PIL image.
    """

    type: Literal["image"]
    image: str | Image.Image


ContentPart = TextPart | ImagePart


Role = str


Content = str | list[ContentPart]


class Message(TypedDict):
    """
    Container for a single turn in a multi-turn conversation.

    Args:

    role: Role
        String that denotes the source of the message, typically system, user, assistant, and tool.
    content: Content
        Content of the message, can be a string, or a list of ContentPart.
    tool_calls: NotRequired[list[ToolCall]]
        Optional sequence of tool calls generated by the model.
    thinking: NotRequired[str]
        Optional thinking produced by the model before its final response.
    trainable: NotRequired[bool]
        Optional indicator whether this message should contribute to the training loss.

    """

    role: Role
    content: Content

    tool_calls: NotRequired[list[ToolCall]]
    thinking: NotRequired[str]
    trainable: NotRequired[bool]
    tool_call_id: NotRequired[str]
    name: NotRequired[str]


def ensure_text(content: Content) -> str:
    """
    Assert that content is text-only and return it as a string.

    Raises ValueError if content contains images or multiple parts.
    Use this to validate that message content is text-only before
    processing it in code paths that don't support multimodal content.
    """
    if isinstance(content, str):
        return content
    if len(content) == 1 and content[0]["type"] == "text":
        return content[0]["text"]
    raise ValueError(f"Expected text content, got multimodal content with {len(content)} parts")


def _tool_call_payload(tool_call: ToolCall) -> dict[str, object]:
    """Minimal JSON payload for embedding in <tool_call> blocks."""
    # Convert from nested structure to flat format for compatibility
    return {
        "name": tool_call.function.name,
        "args": json.loads(tool_call.function.arguments),
    }


class RenderedMessage(TypedDict):
    """
    Container for parts of a rendered message, for masking.

    Args:

    prefix: NotRequired[tinker.EncodedTextChunk]
        Message header that typically includes the speaker's role in the conversation.
    content: list[tinker.ModelInputChunk]
        Inner parts of the message that may include spans of image and text.
    suffix: NotRequired[tinker.EncodedTextChunk]
        Message header that typically includes the turn stop token.

    """

    prefix: NotRequired[tinker.EncodedTextChunk]
    content: list[tinker.ModelInputChunk]
    suffix: NotRequired[tinker.EncodedTextChunk]


class TrainOnWhat(StrEnum):
    LAST_ASSISTANT_MESSAGE = "last_assistant_message"
    ALL_ASSISTANT_MESSAGES = "all_assistant_messages"
    ALL_MESSAGES = "all_messages"
    ALL_TOKENS = "all_tokens"
    ALL_USER_AND_SYSTEM_MESSAGES = "all_user_and_system_messages"
    CUSTOMIZED = "customized"


class Renderer(Protocol):
    """
    Render a message list into training and sampling prompts for language models.
    """

    tokenizer: Tokenizer

    def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer

    def _preprocess_message_parts(self, message: Message) -> list[ImagePart | TextPart]:
        return (
            message["content"]
            if isinstance(message["content"], list)
            else [TextPart(type="text", text=message["content"])]
        )

    @property
    def _bos_tokens(self) -> list[int]:
        return []

    def get_stop_sequences(self) -> list[str] | list[int]:
        raise NotImplementedError

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        raise NotImplementedError

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        raise NotImplementedError

    def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> tinker.ModelInput:
        """
        Generates tokens for sampling from the model.

        Args:
            messages: a list of messages to render.
            role: the role of the partial message to be completed.
            prefill: an optional string to prefill in the model's generation.
        """

        chunks: list[tinker.types.ModelInputChunk] = []
        if self._bos_tokens:
            chunks.append(tinker.types.EncodedTextChunk(tokens=self._bos_tokens))
        for idx, message in enumerate(messages):
            rendered_message = self.render_message(idx, message)
            ob_chunk = rendered_message.get("prefix")
            action_chunks = rendered_message["content"]
            if ob_chunk:
                chunks.append(ob_chunk)
            chunks.extend([x for x in action_chunks if x])
        new_partial_message = Message(role=role, content="")
        rendered_message = self.render_message(len(messages), new_partial_message)
        ob_chunk = rendered_message.get("prefix")
        if ob_chunk:
            chunks.append(ob_chunk)
        if prefill:
            chunks.append(
                tinker.types.EncodedTextChunk(
                    tokens=self.tokenizer.encode(prefill, add_special_tokens=False)
                )
            )
        return tinker.ModelInput(chunks=chunks)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[tinker.ModelInput, torch.Tensor]:
        """
        Generates tokens and weights (for SFT) in the most standard way; by concatenating
        together tokens and weights for each message.

        Args:
            messages: a list of messages to render.
            train_on_what: an enum that controls how the weights are assigned to the tokens.
                - TrainOnWhat.LAST_ASSISTANT_MESSAGE: only the last assistant message is used for training
                - TrainOnWhat.ALL_ASSISTANT_MESSAGES: all assistant messages are used for training
                - TrainOnWhat.ALL_MESSAGES: all messages are used for training
                - TrainOnWhat.ALL_TOKENS: all tokens are used for training
                - TrainOnWhat.ALL_USER_AND_SYSTEM_MESSAGES: all user and system messages are used for training
                - TrainOnWhat.CUSTOMIZED: each message has a trainable field, and the weights are assigned based on the trainable field

        Returns:
            A tuple of two tensors:
                - model_input: the tinker ModelInput for your model
                - weights: a tensor of weights
        """

        model_input_chunks_weights: list[tuple[tinker.types.ModelInputChunk, float]] = []
        if self._bos_tokens:
            model_input_chunks_weights.append(
                (tinker.types.EncodedTextChunk(tokens=self._bos_tokens), 0.0)
            )

        for idx, message in enumerate(messages):
            if train_on_what == TrainOnWhat.CUSTOMIZED:
                assert "trainable" in message, (
                    "When using CUSTOMIZED train_on_what, each message must have a trainable field: True if loss is applied on this message, False otherwise"
                )
            else:
                assert "trainable" not in message, (
                    "When using non-CUSTOMIZED train_on_what, each message must not have a trainable field. Either change train_on_what to CUSTOMIZED or remove the trainable field from the message"
                )

            is_last_message = idx == len(messages) - 1
            is_assistant = message["role"] == "assistant"
            is_user_or_system = message["role"] in ["user", "system"]

            # only apply weight to observation part if train_on_what is ALL_TOKENS
            rendered_message = self.render_message(idx, message, is_last=is_last_message)
            ob_part = rendered_message.get("prefix")
            action_parts = rendered_message.get("content")
            action_tail = rendered_message.get("suffix")

            ob_weight = int(train_on_what == TrainOnWhat.ALL_TOKENS)
            if ob_part:
                model_input_chunks_weights += [(ob_part, ob_weight)]

            match train_on_what:
                case TrainOnWhat.LAST_ASSISTANT_MESSAGE:
                    action_has_weight = is_last_message and is_assistant
                case TrainOnWhat.ALL_ASSISTANT_MESSAGES:
                    action_has_weight = is_assistant
                case TrainOnWhat.ALL_MESSAGES:
                    action_has_weight = True
                case TrainOnWhat.ALL_TOKENS:
                    action_has_weight = True
                case TrainOnWhat.ALL_USER_AND_SYSTEM_MESSAGES:
                    action_has_weight = is_user_or_system
                case TrainOnWhat.CUSTOMIZED:
                    action_has_weight = message.get("trainable", False)
                case _:
                    raise ValueError(f"Unknown train_on_what: {train_on_what}")

            model_input_chunks_weights += [
                (action_part, int(action_has_weight)) for action_part in action_parts if action_part
            ]

            # action tail is effectively the stop_token and the start token for the next turn
            # e.g. \n\nUser:
            if is_last_message and action_tail:
                model_input_chunks_weights += [(action_tail, int(action_has_weight))]

        weights_data = [w for chunk, w in model_input_chunks_weights for _ in range(chunk.length)]
        weights_tensor = torch.tensor(weights_data)

        model_input_chunks = [chunk for chunk, _ in model_input_chunks_weights]
        return tinker.ModelInput(chunks=model_input_chunks), weights_tensor


def tokens_weights_from_strings_weights(
    strings_weights: list[tuple[str, float]],
    tokenizer: Tokenizer,
) -> tuple[torch.Tensor, torch.Tensor]:
    strings, weights = zip(*strings_weights, strict=True)
    token_chunks = [tokenizer.encode(s, add_special_tokens=i == 0) for i, s in enumerate(strings)]
    weights = torch.cat(
        [torch.full((len(chunk),), w) for chunk, w in zip(token_chunks, weights, strict=True)]
    )
    tokens = torch.cat([torch.tensor(chunk) for chunk in token_chunks])
    assert tokens.dtype == torch.int64
    return tokens, weights


def parse_response_for_stop_token(
    response: list[int], tokenizer: Tokenizer, stop_token: int
) -> tuple[Message, bool]:
    """Parse response for a single stop token.

    We expect a properly rendered response to have exactly one stop token; but it may have zero if e.g. the model
    ran out of tokens when sampling, which will incur a format error. If there are > 1, there is likely a bug in the
    sampler and we should error.
    """
    emt_count = response.count(stop_token)
    if emt_count == 0:
        str_response = tokenizer.decode(response)
        logger.debug(f"Response is not a valid assistant response: {str_response}")
        return Message(role="assistant", content=str_response), False
    elif emt_count == 1:
        str_response = tokenizer.decode(response[: response.index(stop_token)])
        return Message(role="assistant", content=str_response), True
    else:
        raise ValueError(
            f"When parsing response, expected to split into 1 or 2 pieces using stop tokens, but got {emt_count}. "
            "You probably are using the wrong stop tokens when sampling"
        )


class ImageProcessorProtocol(Protocol):
    merge_size: int
    patch_size: int

    def get_number_of_image_patches(
        self, height: int, width: int, images_kwargs: Optional[dict] = None
    ) -> int:
        raise NotImplementedError()


def image_to_chunk(
    image_or_str: Image.Image | str, image_processor: ImageProcessorProtocol
) -> tinker.types.ImageChunk:
    """
    Convert a PIL Image to a tinker.types.ImageChunk for QwenVL
    """

    # load an image from a data URI or a URL
    if isinstance(image_or_str, str):
        with urllib.request.urlopen(image_or_str) as response:
            pil_image = Image.open(io.BytesIO(response.read()))

    # Otherwise the image is a PIL image and can be loaded directly
    elif isinstance(image_or_str, Image.Image):
        pil_image = image_or_str

    # Validate the provided data is actually a valid image type
    else:
        raise ValueError("The provided image must be a PIL.Image.Image, URL, or data URI.")

    # Convert to RGB if needed (JPEG doesn't support RGBA/LA/P modes)
    if pil_image.mode in ("RGBA", "LA", "P"):
        pil_image = pil_image.convert("RGB")

    img_byte_arr = io.BytesIO()
    pil_image.save(img_byte_arr, format="JPEG")
    image_data = img_byte_arr.getvalue()

    width, height = pil_image.size
    num_image_tokens = (
        image_processor.get_number_of_image_patches(height, width, images_kwargs={})
        // image_processor.merge_size**2
    )

    return tinker.types.ImageChunk(
        data=image_data,
        format="jpeg",
        expected_tokens=num_image_tokens,
    )

